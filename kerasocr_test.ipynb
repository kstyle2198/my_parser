{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --force-reinstall -v \"tensorflow==2.15.1\"\n",
    "# pip install keras-ocr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_ocr\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawBoxes(image, boxes, color=(255, 0, 0), thickness=5, boxes_format=\"boxes\"):\n",
    "    \"\"\"Draw boxes onto an image.\n",
    "\n",
    "    Args:\n",
    "        image: The image on which to draw the boxes.\n",
    "        boxes: The boxes to draw.\n",
    "        color: The color for each box.\n",
    "        thickness: The thickness for each box.\n",
    "        boxes_format: The format used for providing the boxes. Options are\n",
    "            \"boxes\" which indicates an array with shape(N, 4, 2) where N is the\n",
    "            number of boxes and each box is a list of four points) as provided\n",
    "            by `keras_ocr.detection.Detector.detect`, \"lines\" (a list of\n",
    "            lines where each line itself is a list of (box, character) tuples) as\n",
    "            provided by `keras_ocr.data_generation.get_image_generator`,\n",
    "            or \"predictions\" where boxes is by itself a list of (word, box) tuples\n",
    "            as provided by `keras_ocr.pipeline.Pipeline.recognize` or\n",
    "            `keras_ocr.recognition.Recognizer.recognize_from_boxes`.\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return image\n",
    "    canvas = image.copy()\n",
    "    if boxes_format == \"lines\":\n",
    "        revised_boxes = []\n",
    "        for line in boxes:\n",
    "            for box, _ in line:\n",
    "                revised_boxes.append(box)\n",
    "        boxes = revised_boxes\n",
    "    if boxes_format == \"predictions\":\n",
    "        revised_boxes = []\n",
    "        for _, box in boxes:\n",
    "            revised_boxes.append(box)\n",
    "        boxes = revised_boxes\n",
    "    for box in boxes:\n",
    "        cv2.polylines(\n",
    "            img=canvas,\n",
    "            pts=box[np.newaxis].astype(\"int32\"),\n",
    "            color=color,\n",
    "            thickness=thickness,\n",
    "            isClosed=True,\n",
    "        )\n",
    "    return canvas\n",
    "\n",
    "def drawAnnotations(image, predictions, ax=None):\n",
    "    \"\"\"Draw text annotations onto image.\n",
    "    Args:\n",
    "        image: The image on which to draw\n",
    "        predictions: The predictions as provided by `pipeline.recognize`.\n",
    "        ax: A matplotlib axis on which to draw.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.imshow(drawBoxes(image=image, boxes=predictions, boxes_format=\"predictions\"))\n",
    "    predictions = sorted(predictions, key=lambda p: p[1][:, 1].min())\n",
    "    left = []\n",
    "    right = []\n",
    "    for word, box in predictions:\n",
    "        if box[:, 0].min() < image.shape[1] / 2:\n",
    "            left.append((word, box))\n",
    "        else:\n",
    "            right.append((word, box))\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    for side, group in zip([\"left\", \"right\"], [left, right]):\n",
    "        for index, (text, box) in enumerate(group):\n",
    "            y = 1 - (index / len(group))\n",
    "            xy = box[0] / np.array([image.shape[1], image.shape[0]])\n",
    "            xy[1] = 1 - xy[1]\n",
    "            ax.annotate(\n",
    "                text=text,\n",
    "                xy=xy,\n",
    "                xytext=(-0.05 if side == \"left\" else 1.05, y),\n",
    "                xycoords=\"axes fraction\",\n",
    "                arrowprops={\"arrowstyle\": \"->\", \"color\": \"r\"},\n",
    "                color=\"r\",\n",
    "                fontsize=14,\n",
    "                horizontalalignment=\"right\" if side == \"left\" else \"left\",\n",
    "            )\n",
    "    # st.pyplot(fig)\n",
    "    return ax\n",
    "\n",
    "class CustomDrawingParser():\n",
    "    def __int__(self):\n",
    "        pass\n",
    "\n",
    "    # @st.experimental_fragment\n",
    "    def keras_ocr_extract_prediction(self, image_path):\n",
    "        pipeline = keras_ocr.pipeline.Pipeline()\n",
    "        image = keras_ocr.tools.read(image_path)\n",
    "        predictions = pipeline.recognize([image])[0]\n",
    "        return predictions\n",
    "\n",
    "    # @st.experimental_fragment\n",
    "    def show_drawings(self, image_path, predictions):\n",
    "        image = keras_ocr.tools.read(image_path)\n",
    "        result = drawAnnotations(image=image, predictions=predictions)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomDrawingParser at 0x1953b65d870>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdp = CustomDrawingParser()\n",
    "cdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for C:\\Users\\jongb\\.keras-ocr\\craft_mlt_25k.h5\n",
      "Looking for C:\\Users\\jongb\\.keras-ocr\\crnn_kurapan.h5\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/AA_develop/parsing/images/개정도_표지2/개정도_표지2_0.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mcdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras_ocr_extract_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m prediction\n",
      "Cell \u001b[1;32mIn[6], line 88\u001b[0m, in \u001b[0;36mCustomDrawingParser.keras_ocr_extract_prediction\u001b[1;34m(self, image_path)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkeras_ocr_extract_prediction\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_path):\n\u001b[0;32m     87\u001b[0m     pipeline \u001b[38;5;241m=\u001b[39m keras_ocr\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mPipeline()\n\u001b[1;32m---> 88\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_ocr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mrecognize([image])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32md:\\AA_develop\\parsing\\myenv_310\\lib\\site-packages\\keras_ocr\\tools.py:38\u001b[0m, in \u001b[0;36mread\u001b[1;34m(filepath_or_buffer)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(filepath_or_buffer), (\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find image at path: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m filepath_or_buffer\n\u001b[0;32m     36\u001b[0m     )\n\u001b[0;32m     37\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(filepath_or_buffer)\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "img_path = \"D:/AA_develop/parsing/images/개정도_표지2/개정도_표지2_0.png\"\n",
    "prediction = cdp.keras_ocr_extract_prediction(img_path)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words = [pred[0] for pred in prediction]\n",
    "words = \",\".join(words)\n",
    "words\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
