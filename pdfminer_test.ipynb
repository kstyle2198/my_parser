{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"D:/AA_develop/parsing/2024/Manual/BERT.pdf\"\n",
    "# path = \"D:/AA_develop/parsing/2024/POS/FWG.pdf\"\n",
    "path = \"D:/AA_develop/parsing/2024/Rule/Meta Llama Responsible Use Guide.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta Llama\n",
      "\n",
      "Responsible \n",
      "Use Guide\n",
      "\n",
      "Resources and best practices for \n",
      "\n",
      "responsible development of products \n",
      "\n",
      "built with large language models\n",
      "\n",
      "\fContents\n",
      "\n",
      "Open Innovation \n",
      "\n",
      "  How to use this guide \n",
      "\n",
      "Overview of responsible AI & system design   \n",
      "\n",
      "  Responsible AI considerations \n",
      "\n",
      "  Mitigation points for LLM-powered products \n",
      "\n",
      "Development of the foundation model \n",
      "\n",
      "Responsible LLM product development stages \n",
      "\n",
      "  Determine use case \n",
      "\n",
      "Define content policies  \n",
      "\n",
      "Understand alignment-helpfulness trade-offs \n",
      "\n",
      "  Model-level alignment \n",
      "\n",
      "Step 1: Prepare data \n",
      "\n",
      "  Step 2: Train the model \n",
      "\n",
      "Reinforcement Learning from Human Feedback (RLHF) \n",
      "\n",
      "1\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "7\n",
      "\n",
      " 8\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "10\n",
      "\n",
      "11\n",
      "\n",
      "12\n",
      "\n",
      "Reinforcement Learning from AI Feedback (RLAIF) \n",
      "\n",
      "               12\n",
      "\n",
      "  Step 3: Evaluate and improve performance  \n",
      "\n",
      "Red teaming best practices \n",
      "\n",
      "Privacy adversarial attacks \n",
      "\n",
      "System-level alignment \n",
      "\n",
      "  Mitigating risks at the input level \n",
      "\n",
      "  Mitigating risks at the output level \n",
      "\n",
      "  Evaluate effectiveness  \n",
      "\n",
      "12\n",
      "\n",
      "13\n",
      "\n",
      "14\n",
      "\n",
      "14\n",
      "\n",
      "15\n",
      "\n",
      "16\n",
      "\n",
      "17\n",
      "\n",
      "Build transparency and reporting mechanisms  \n",
      "in user interactions \n",
      "\n",
      "               17\n",
      "\n",
      "  Feedback & reporting mechanisms  \n",
      "\n",
      "  Transparency & control best practices  \n",
      "\n",
      "Resources for developers \n",
      "\n",
      "Combining the components of responsible generative AI \n",
      "\n",
      "Addendum: Introducing Code Llama \n",
      "\n",
      "Foundation model use case \n",
      "\n",
      "Instruction model use case \n",
      "\n",
      "17\n",
      "\n",
      "18\n",
      "\n",
      "19\n",
      "\n",
      "21\n",
      "\n",
      "22\n",
      "\n",
      "23\n",
      "\n",
      "25\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fMeta is committed to open science because  \n",
      "\n",
      "we believe that a vibrant AI-innovation \n",
      "\n",
      "ecosystem will push the frontiers of scientific \n",
      "\n",
      "discovery and potentially revolutionize a wide \n",
      "\n",
      "array of sectors from education to agriculture, \n",
      "\n",
      "and climate management to cybersecurity. \n",
      "\n",
      "We believe that the power of AI will be harnessed to address global \n",
      "\n",
      "challenges, and unlocking that power responsibly will require \n",
      "\n",
      "democratization of access and collaboration on risk management. \n",
      "\n",
      "We want to empower developers in every industry on a global \n",
      "\n",
      "scale to drive breakthroughs, create new products and solutions, \n",
      "\n",
      "and benefit from accelerations in technological advancement and \n",
      "\n",
      "economic growth.\n",
      "\n",
      "1\n",
      "\n",
      "April 2024AI at Meta\fMeta has open sourced code and datasets for \n",
      "\n",
      "Meta is proud to have supported the Llama 2 developer \n",
      "\n",
      "machine translation, computer vision, and fairness \n",
      "\n",
      "community by building state-of-the-art responsibility \n",
      "\n",
      "evaluation, while contributing to the infrastructure  \n",
      "\n",
      "tooling that makes it easier than ever to build and release \n",
      "\n",
      "of the AI-developer community with tools like \n",
      "\n",
      "models responsibly. Learn more about the open source \n",
      "\n",
      "PyTorch, ONNX, Glow, and Detectron. In the past, \n",
      "\n",
      "tools we share with developers to help them build \n",
      "\n",
      "we have also made our cutting-edge large language \n",
      "\n",
      "responsibly.\n",
      "\n",
      "models (LLMs) Llama 1 and OPT-175B available to  \n",
      "\n",
      "the scientific community through research releases \n",
      "\n",
      "which have spurred research in model efficiency, \n",
      "\n",
      "medicine, and conversational safety studies on \n",
      "\n",
      "evaluation methods, de-biasing techniques, and \n",
      "\n",
      "sources of hallucinations in LLMs. \n",
      "\n",
      "We also took an important step toward advancing \n",
      "\n",
      "access and opportunity in the creation of AI-powered \n",
      "\n",
      "products and experiences with the launch of Meta \n",
      "\n",
      "Llama 2. The open release of these models to the \n",
      "\n",
      "research and business community laid the foundation \n",
      "\n",
      "for the next wave of community-driven innovation in \n",
      "\n",
      "generative AI. We’ve seen an incredible response  \n",
      "\n",
      "thus far with millions of download requests in the \n",
      "\n",
      "time since its release.\n",
      "\n",
      "We’re also excited to release an early look at the next \n",
      "\n",
      "generation of Llama, Meta Llama 3 which, like Llama 2,  \n",
      "\n",
      "is licensed for commercial use. This release of  \n",
      "\n",
      "Llama 3 features both 8B and 70B pretrained and \n",
      "\n",
      "instruct fine-tuned versions to help support a broad \n",
      "\n",
      "range of application environments.\n",
      "\n",
      "We envision Llama models \n",
      "\n",
      "as part of a broader system \n",
      "\n",
      "that puts the developer in the \n",
      "\n",
      "driver seat. \n",
      "\n",
      "Llama models will serve as the foundational piece of  \n",
      "\n",
      "a complex system that developers design with their \n",
      "\n",
      "Democratization of access will put these models  \n",
      "\n",
      "unique end goals in mind. As part of this system \n",
      "\n",
      "in more people’s hands, which we believe is the  \n",
      "\n",
      "centric approach, and to support responsible \n",
      "\n",
      "right path to ensure that this technology will benefit \n",
      "\n",
      "deployment of these models, we have updates to our \n",
      "\n",
      "the world at large. We take our commitment to \n",
      "\n",
      "open trust and safety project including a Meta Llama \n",
      "\n",
      "building responsible AI seriously, cognizant of the \n",
      "\n",
      "Guard 2 model that supports a broader taxonomy for \n",
      "\n",
      "potential privacy and content-related risks, as well  \n",
      "\n",
      "input/output prompt filtering.\n",
      "\n",
      "as societal impacts. \n",
      "\n",
      "2\n",
      "\n",
      "April 2024AI at Meta\fHow to use this guide\n",
      "\n",
      "This guide is a resource for developers that outlines \n",
      "\n",
      "The recommendations included in this guide reflect \n",
      "\n",
      "common approaches to building responsibly at each \n",
      "\n",
      "current research on responsible generative AI. We \n",
      "\n",
      "level of an LLM-powered product. It covers best \n",
      "\n",
      "expect these to evolve as the field advances and \n",
      "\n",
      "practices and considerations that developers should \n",
      "\n",
      "access to foundation models grows, inviting further \n",
      "\n",
      "evaluate in the context of their specific use case and \n",
      "\n",
      "innovation on AI safety. Decisions to implement \n",
      "\n",
      "market. It also highlights some mitigation strategies \n",
      "\n",
      "best practices should be evaluated based on the \n",
      "\n",
      "and resources available to developers to address risks \n",
      "\n",
      "jurisdiction where your products will be deployed and \n",
      "\n",
      "at various points in the system. These best practices \n",
      "\n",
      "should follow your company’s internal legal and risk \n",
      "\n",
      "should be considered holistically because strategies \n",
      "\n",
      "management processes.\n",
      "\n",
      "adopted at one level can impact the entire system. \n",
      "\n",
      "3\n",
      "\n",
      "April 2024AI at Meta\fOverview of responsible \n",
      "AI & system design\n",
      "\n",
      "Responsible AI considerations \n",
      "\n",
      "Helping to ensure that generative AI technology \n",
      "\n",
      "technology that have already surfaced online, such as \n",
      "\n",
      "does not produce content that could cause harm is of \n",
      "\n",
      "the creation or proliferation of illegal content, content \n",
      "\n",
      "paramount importance. Generative AI is developing \n",
      "\n",
      "which may be objectionable or hateful, or content \n",
      "\n",
      "rapidly and is being driven by research, open \n",
      "\n",
      "that may result in the provision of unqualified advice. \n",
      "\n",
      "collaboration, and product releases that are putting \n",
      "\n",
      "These instances may increase as generative AI tools \n",
      "\n",
      "this technology in the hands of people globally. \n",
      "\n",
      "become more accessible.\n",
      "\n",
      "Growth at this scale presents novel challenges for \n",
      "\n",
      "the responsible deployment of AI, yet many of the \n",
      "\n",
      "principles of responsibility remain the same as for any \n",
      "\n",
      "other AI technology. These considerations, core to \n",
      "\n",
      "Meta’s approach to responsible AI, include fairness \n",
      "\n",
      "and inclusion, robustness and safety, privacy and \n",
      "\n",
      "security, and transparency and control, as well as \n",
      "\n",
      "mechanisms for governance and accountability. LLMs \n",
      "\n",
      "are one of many AI tools, and their risks should be \n",
      "\n",
      "evaluated through these lenses according to how they \n",
      "\n",
      "will be used. \n",
      "\n",
      "Foundation models and generative AI systems \n",
      "\n",
      "represent advancements in power and accuracy \n",
      "\n",
      "compared to predecessor technologies. The increase \n",
      "\n",
      "in the performance, utility, and flexibility of these \n",
      "\n",
      "models will likely lead to their ubiquity, as the value \n",
      "\n",
      "they bring to some pre-existing use cases may \n",
      "\n",
      "outweigh operational costs of deploying the systems. \n",
      "\n",
      "The ability to generate completely new content also \n",
      "\n",
      "opens up new use cases that must be evaluated \n",
      "\n",
      "for the types of risks they may present. There are \n",
      "\n",
      "potential risks related to the misuse of this \n",
      "\n",
      "For our own, on-platform generative AI offerings, \n",
      "\n",
      "Meta is implementing safety measures to address \n",
      "\n",
      "context-specific risks. These mitigations are layered \n",
      "\n",
      "across different intervention points beyond those \n",
      "\n",
      "that can be assessed and mitigated in the foundation \n",
      "\n",
      "model. With our release of Llama 3 paired with Llama \n",
      "\n",
      "Guard 2, we are beginning to extend this vision of a \n",
      "\n",
      "layered approach to safety to our open models \n",
      "\n",
      " as well.\n",
      "\n",
      "As discussed in our research paper on Llama 2, \n",
      "\n",
      "some mitigations applied at early stages in the \n",
      "\n",
      "development process can be detrimental to the \n",
      "\n",
      "performance and safety of the model, and some \n",
      "\n",
      "risks may be better addressed at later points in the \n",
      "\n",
      "product development cycle. Our vision for layered \n",
      "\n",
      "model safety helps to empower developers to \n",
      "\n",
      "make decisions about balancing these trade-offs. \n",
      "\n",
      "Developers of generative AI-powered features that \n",
      "\n",
      "leverage open source models will have more power to \n",
      "\n",
      "ensure that their products are safe and benefit end \n",
      "\n",
      "users, while taking a holistic view of responsible AI \n",
      "\n",
      "across the entire product development cycle. \n",
      "\n",
      "4\n",
      "\n",
      "April 2024AI at Meta\fMitigation points for LLM-\n",
      "powered products\n",
      "\n",
      "A foundation model is a general purpose AI \n",
      "\n",
      "technology whereas an LLM-powered product has \n",
      "\n",
      "a defined use case and performs specific tasks \n",
      "\n",
      "to enable an intended use or capability through a \n",
      "\n",
      "user interface, sometimes embedded in products. \n",
      "\n",
      "Model-level safety: Model-level safety concerns the \n",
      "\n",
      "data preparation and processing best practices and \n",
      "\n",
      "human feedback or alignment practices for safety at \n",
      "\n",
      "the foundation and fine-tuned model level.\n",
      "\n",
      "An LLM-powered system encompasses both the \n",
      "\n",
      "System-level safety: System-level safety is the venue \n",
      "\n",
      "foundation model and accompanying input-output \n",
      "\n",
      "for the most context-specific safety mitigations \n",
      "\n",
      "safeguards, and a number of product-specific \n",
      "\n",
      "dependent on user interactions. Developers looking \n",
      "\n",
      "layers. At various points in the product development \n",
      "\n",
      "to craft safety mitigations specifically for their use \n",
      "\n",
      "lifecycle, developers make decisions that shape the \n",
      "\n",
      "case with the goal of offering their users the best \n",
      "\n",
      "objectives and functionality of the feature, which can \n",
      "\n",
      "product experience should explore these options.\n",
      "\n",
      "introduce potential risks. These decision points also \n",
      "\n",
      "provide opportunities to mitigate potential risks. It \n",
      "\n",
      "is critical that developers examine each layer of the \n",
      "\n",
      "product to determine which potential risks may arise \n",
      "\n",
      "You can learn more about our layered approach to \n",
      "\n",
      "safety by visiting our resources for Meta Llama open \n",
      "\n",
      "trust and safety.\n",
      "\n",
      "based on the product objectives and design, and \n",
      "\n",
      "The following section presents responsible AI \n",
      "\n",
      "implement mitigation strategies accordingly. \n",
      "\n",
      "considerations for the different stages of LLM \n",
      "\n",
      "product development. At each of these levels, we \n",
      "\n",
      "highlight best practices for mitigating potential risks. \n",
      "\n",
      "5\n",
      "\n",
      "April 2024AI at Meta\fDevelopment of the \n",
      "foundation model\n",
      "\n",
      "Meta Llama 3, like Llama 2, is licensed for commercial \n",
      "\n",
      "The training datasets for Llama are sourced from \n",
      "\n",
      "use. This release of Llama 3 features both 8B and 70B \n",
      "\n",
      "a broad set of diverse, publicly available online \n",
      "\n",
      "pretrained and instruct fine-tuned versions to help \n",
      "\n",
      "data. This training corpus is mostly English, which \n",
      "\n",
      "support a broad range of application environments. \n",
      "\n",
      "is consistent with the current, intended use of \n",
      "\n",
      "This next generation of Llama demonstrates state-\n",
      "\n",
      "the model. For each dataset used in training, we \n",
      "\n",
      "of-the-art performance on a wide range of industry \n",
      "\n",
      "followed Meta’s standard privacy review processes. \n",
      "\n",
      "benchmarks and offers new capabilities, including \n",
      "\n",
      "And for our pretraining data we made an effort \n",
      "\n",
      "improved reasoning. With the developer in mind, and \n",
      "\n",
      "to remove data from certain sources known to \n",
      "\n",
      "in support of our longstanding open approach, we \n",
      "\n",
      "contain a high volume of personal information about \n",
      "\n",
      "wanted to put Llama 3 in the hands of the community \n",
      "\n",
      "private individuals. After pretraining, the model can \n",
      "\n",
      "as soon as possible to enable early development and \n",
      "\n",
      "reproduce everything from simple grammatical rules \n",
      "\n",
      "kickstart this next wave of innovation.\n",
      "\n",
      "to complex nuances like context, sentiment, and \n",
      "\n",
      "In addition to performing a variety of pretraining \n",
      "\n",
      "data-level investigations to help understand the \n",
      "\n",
      "potential capabilities and limitations of our models, \n",
      "\n",
      "we applied considerable safety mitigations to the \n",
      "\n",
      "fine-tuned versions of the model through supervised \n",
      "\n",
      "figurative language. However, the model does not \n",
      "\n",
      "gain knowledge or generate beliefs about the world \n",
      "\n",
      "in the way humans do. It only learns to predict the \n",
      "\n",
      "next word in a sentence based on the patterns in its \n",
      "\n",
      "training data. \n",
      "\n",
      "fine-tuning, reinforcement learning from human \n",
      "\n",
      "If you’re going to use the pretrained model, we \n",
      "\n",
      "feedback (RLHF), and iterative red teaming (these \n",
      "\n",
      "recommend tuning it by using the techniques \n",
      "\n",
      "steps are covered further in the section - Fine-tune \n",
      "\n",
      "described in the next section to reduce the likelihood \n",
      "\n",
      "for product).  \n",
      "\n",
      "More information on Llama 3 model architecture and \n",
      "\n",
      "parameters and pretrained evaluations are contained \n",
      "\n",
      "in the model card. The model card also provides \n",
      "\n",
      "information about the capabilities and limitations of \n",
      "\n",
      "the models. \n",
      "\n",
      "that the model will generate outputs that are in \n",
      "\n",
      "conflict with your intended use case and tasks. If \n",
      "\n",
      "you have terms of service or other relevant policies \n",
      "\n",
      "that apply to how individuals may interact with your \n",
      "\n",
      "LLM, you may wish to fine-tune your model to be \n",
      "\n",
      "aligned with those policies. It may also be necessary \n",
      "\n",
      "to establish new terms of service and policies specific \n",
      "\n",
      "During pretraining, a model builds its understanding \n",
      "\n",
      "to LLMs, or notify users about how their data or \n",
      "\n",
      "of the statistical patterns across the sample of \n",
      "\n",
      "feedback provided will be used in fine-tuning. We also \n",
      "\n",
      "human language contained in its training data.  \n",
      "\n",
      "recommend using Llama Guard 2 for enhanced safety \n",
      "\n",
      "performance. \n",
      "\n",
      "6\n",
      "\n",
      "April 2024AI at Meta\fResponsible LLM product \n",
      "development stages\n",
      "\n",
      "Developers will identify a specific product use case \n",
      "\n",
      "for the released model, and are responsible for \n",
      "\n",
      "assessing risks associated with that use case and \n",
      "\n",
      "applying best practices to ensure safety. This section \n",
      "\n",
      "is which use case(s) to focus on. Most developers \n",
      "\n",
      "outlines the considerations and mitigation strategies \n",
      "\n",
      "using this guide already have a use case in mind, \n",
      "\n",
      "available at each stage of product development  \n",
      "\n",
      "such as customer support, AI assistants, internal \n",
      "\n",
      "An important decision in the development process \n",
      "\n",
      "1Determine use case\n",
      "\n",
      "productivity tools, entertaining end-user experiences, \n",
      "\n",
      "or research applications. If you’re a developer who \n",
      "\n",
      "is not certain of a particular use case for which you \n",
      "\n",
      "would want to use the model, consider focusing on \n",
      "\n",
      "use cases that improve the lives of people and society, \n",
      "\n",
      "taking into consideration different ethical principles \n",
      "\n",
      "and values. Developing or adopting an internal risk \n",
      "\n",
      "assessment process can help identify potential \n",
      "\n",
      "risks for a specific use case and should focus on \n",
      "\n",
      "and deployment. \n",
      "\n",
      "At a high level these stages include: \n",
      "\n",
      "1.  Determine use case\n",
      "\n",
      "2.  Model-level alignment\n",
      "\n",
      "3.  System-level alignment\n",
      "\n",
      "4.  Build transparency and reporting \n",
      "\n",
      "how your product’s end users and others could be \n",
      "\n",
      "mechanisms in user interactions\n",
      "\n",
      "affected. This understanding is critical for evaluating \n",
      "\n",
      "in-context safety for your product deployment, and \n",
      "\n",
      "can take forms such as surveys and interviews of \n",
      "\n",
      "potential users or market analysis of similar product \n",
      "\n",
      "applications.\n",
      "\n",
      "If you are new to considerations of values in the \n",
      "\n",
      "development and deployment of AI, refer to the \n",
      "\n",
      "principles and guidance on risk management released \n",
      "\n",
      "by academic and expert institutions, such as: \n",
      "\n",
      "•  OECD’s AI Principles \n",
      "\n",
      "•  NIST’s Trustworthy and Responsible AI  \n",
      "\n",
      "Resource Center\n",
      "\n",
      "7\n",
      "\n",
      "April 2024AI at Meta\fDefine content policies \n",
      "\n",
      "Based on the intended use and audience for your \n",
      "\n",
      "product, a content policy will define what content \n",
      "\n",
      "is allowable and may outline safety limitations on \n",
      "\n",
      "applying content policies falsely (i.e., false positives \n",
      "\n",
      "and false negatives.) These errors will necessarily \n",
      "\n",
      "mean that a model will either be more aligned and \n",
      "\n",
      "less helpful or less aligned and more helpful.\n",
      "\n",
      "producing illegal, violent, or harmful content. These \n",
      "\n",
      "To illustrate: Consider a content policy against \n",
      "\n",
      "limits should be evaluated in light of the product \n",
      "\n",
      "assistance with scams. If a user submits a prompt \n",
      "\n",
      "domain, as specific sectors and regions may have \n",
      "\n",
      "for “How does a ponzi scheme operate?” the model \n",
      "\n",
      "different laws or standards. Additionally, the needs \n",
      "\n",
      "can either refuse to substantively answer (arguably \n",
      "\n",
      "of specific user communities should be considered as \n",
      "\n",
      "the most aligned, least helpful option) or provide a \n",
      "\n",
      "you design content policies, such as the development \n",
      "\n",
      "complete, detailed answer (arguably the most helpful, \n",
      "\n",
      "of age-appropriate product experiences. Having \n",
      "\n",
      "least aligned option). Consider the same evaluation, \n",
      "\n",
      "these policies in place will dictate the data needed, \n",
      "\n",
      "but with the prompt “How to protect yourself from \n",
      "\n",
      "annotation requirements, and goals for safety fine-\n",
      "\n",
      "identity theft.” \n",
      "\n",
      "tuning, including the types of mitigation steps that \n",
      "\n",
      "As the model’s rate of identifying and stopping \n",
      "\n",
      "will be implemented. Defining these policies will be \n",
      "\n",
      "unaligned content grows, its likelihood of falsely \n",
      "\n",
      "used for labeling data in later stages when using \n",
      "\n",
      "stopping aligned content–and thereby reducing its \n",
      "\n",
      "RLHF and in additional product layers, such as making \n",
      "\n",
      "overall helpfulness–grows in tandem. In other words, \n",
      "\n",
      "enforcement decisions for user inputs and model \n",
      "\n",
      "you’ll need to look elsewhere to learn about stopping \n",
      "\n",
      "outputs.\n",
      "\n",
      "identity theft. Turning down the dial–so that more \n",
      "\n",
      "If you are new to considerations of content policies, \n",
      "\n",
      "unaligned content gets through–will likely have the \n",
      "\n",
      "refer to commonly used policies in the industry such \n",
      "\n",
      "knock-on effect of increasing the likelihood that the \n",
      "\n",
      "as the taxonomy proposed by MLCommons.\n",
      "\n",
      "model generates helpful content. You’ll learn about \n",
      "\n",
      "Understand alignment-helpfulness \n",
      "\n",
      "trade-offs \n",
      "\n",
      "While overall model safety should keep improving \n",
      "\n",
      "as models advance, some trade-off between model \n",
      "\n",
      "helpfulness and model alignment is likely unavoidable. \n",
      "\n",
      "That’s because any prediction–Is this content aligned? \n",
      "\n",
      "Is this content unaligned?–carries at least some risk of \n",
      "\n",
      "protecting your identity from thieves.\n",
      "\n",
      "Avoiding alignment-helpfulness trade-offs is \n",
      "\n",
      "probably impossible. But developers should exercise \n",
      "\n",
      "discretion about how to weigh the benefits of \n",
      "\n",
      "alignment and helpfulness for their specific use case \n",
      "\n",
      "and audience. We look forward to exploring more \n",
      "\n",
      "ways to give developers greater control over this \n",
      "\n",
      "important aspect of model building.\n",
      "\n",
      "8\n",
      "\n",
      "April 2024AI at Meta\f2\n",
      "\n",
      "Model-level alignment \n",
      "\n",
      "Product-specific fine-tuning enables developers to \n",
      "\n",
      "leverage pretrained models or models with some fine-\n",
      "\n",
      "tuning for a specific task requiring only limited data \n",
      "\n",
      "and resources. Even with initial fine-tuning performed \n",
      "\n",
      "by Meta, developers can further train the model with \n",
      "\n",
      "domain-specific datasets to improve quality on their \n",
      "\n",
      "These examples showcase \n",
      "\n",
      "how fine-tuning an LLM \n",
      "\n",
      "can be used to specialize \n",
      "\n",
      "the model’s capabilities for \n",
      "\n",
      "defined use case. Fine-tuning adapts the model to \n",
      "\n",
      "domain- or application-specific requirements and \n",
      "\n",
      "introduces additional layers of safety mitigations. \n",
      "\n",
      "specific use cases, improving \n",
      "\n",
      "its performance and making \n",
      "\n",
      "it more suitable for specific \n",
      "\n",
      "applications. The choice of \n",
      "\n",
      "the foundation model and the \n",
      "\n",
      "task-specific dataset plays a \n",
      "\n",
      "crucial role in achieving the \n",
      "\n",
      "desired results.\n",
      "\n",
      "Examples of fine-tuning for a pretrained LLM include:\n",
      "\n",
      "•  Text summarization: By using a pretrained language \n",
      "\n",
      "model, the model can be fine-tuned on a dataset \n",
      "\n",
      "that includes pairs of long-form documents and \n",
      "\n",
      "corresponding summaries. This fine-tuned model \n",
      "\n",
      "can then generate concise summaries for new \n",
      "\n",
      "documents.\n",
      "\n",
      "•  Question answering: Fine-tuning a language \n",
      "\n",
      "model on a Q&A dataset such as SQuAD (Stanford \n",
      "\n",
      "Question Answering Dataset) allows the model to \n",
      "\n",
      "learn how to answer questions based on a given \n",
      "\n",
      "context paragraph. The fine-tuned model can then \n",
      "\n",
      "be used to answer questions on various topics.\n",
      "\n",
      "•  Sentiment analysis: A model can be fine-tuned  \n",
      "\n",
      "on a dataset of labeled text reviews (positive  \n",
      "\n",
      "or negative sentiment) to recognize sentiment and \n",
      "\n",
      "perform analysis to understand user satisfaction. \n",
      "\n",
      "By training the model on this task-specific dataset, \n",
      "\n",
      "it can learn to predict sentiment in text accurately.\n",
      "\n",
      "9\n",
      "\n",
      "April 2024AI at Meta\fThe responsible fine-tuning flow\n",
      "\n",
      "Here are the general steps needed to responsibly fine-\n",
      "\n",
      "tune an LLM for alignment, guided at a high  \n",
      "\n",
      "level by Meta’s Responsible AI framework:\n",
      "\n",
      "1.  Prepare data  \n",
      "\n",
      "2.  Train the model\n",
      "\n",
      "3.  Evaluate and improve performance \n",
      "\n",
      "Step 1: Prepare data \n",
      "\n",
      "Developing downstream applications of LLMs begins \n",
      "\n",
      "with taking steps to consider the potential limitations, \n",
      "\n",
      "privacy implications, and representativeness of \n",
      "\n",
      "data for a specific use case. Begin by preparing and \n",
      "\n",
      "preprocessing a clean dataset that is representative \n",
      "\n",
      "of the target domain. This involves tokenizing the text, \n",
      "\n",
      "handling special characters, removing unnecessary \n",
      "\n",
      "information, and splitting the dataset into training, \n",
      "\n",
      "validation, and testing sets. This step may also involve \n",
      "\n",
      "ensuring that data are representative of the end users \n",
      "\n",
      "in the deployment context, for instance, by ensuring \n",
      "\n",
      "there are enough examples from relevant languages if \n",
      "\n",
      "you plan to deploy your product in a  \n",
      "\n",
      "non-English speaking market. Representativeness \n",
      "\n",
      "of data is dependent on the use case and should be \n",
      "\n",
      "assessed accordingly. \n",
      "\n",
      "When fine-tuning for a specific use case it can be \n",
      "\n",
      "beneficial to examine training data for biases, such \n",
      "\n",
      "as gender, racial, linguistic, cultural or other biases. \n",
      "\n",
      "10\n",
      "\n",
      "April 2024AI at Meta\fTHE RESPONSIBLE FINE-TUNING FLOW\n",
      "\n",
      "Understanding these patterns is important but it may \n",
      "\n",
      " Step 2: Train the model\n",
      "\n",
      "not always be optimal to filter out all problematic \n",
      "\n",
      "content in training data due to the unintended \n",
      "\n",
      "consequences this filtering may have on subsequent \n",
      "\n",
      "performance and safety mitigations, such as prompt \n",
      "\n",
      "engineering. Instead of removing data, focusing on \n",
      "\n",
      "the representativeness of the data can help prevent \n",
      "\n",
      "a fine-tuned model from perpetuating biases in its \n",
      "\n",
      "generated outputs; what is considered representative \n",
      "\n",
      "will depend on the specific context in which a product \n",
      "\n",
      "is deployed. Developers should also pay attention \n",
      "\n",
      "to how human feedback and annotation of data may \n",
      "\n",
      "further polarize a fine-tuned model with respect \n",
      "\n",
      "to subjective opinions, and take steps to prevent \n",
      "\n",
      "injecting bias in annotation guidelines and to  \n",
      "\n",
      "mitigate the effect of annotators’ bias. Resources  \n",
      "\n",
      "on this topic include:\n",
      "\n",
      "Fine-tuning involves training the model for a limited \n",
      "\n",
      "number of iterations. Once a pretrained model \n",
      "\n",
      "is loaded in the environment for fine-tuning, the \n",
      "\n",
      "training process involves setting up hyperparameters \n",
      "\n",
      "like epochs, batch size, and learning rate. The data \n",
      "\n",
      "are passed through the model, loss is computed, and \n",
      "\n",
      "weights are updated through backpropagation. The \n",
      "\n",
      "training progress is monitored using a validation set, \n",
      "\n",
      "and hyperparameters are adjusted as necessary.\n",
      "\n",
      "Fine-tuning an LLM for safety can involve a number \n",
      "\n",
      "of techniques, many of which the research paper on \n",
      "\n",
      "Llama 2 describes in greater depth. These techniques \n",
      "\n",
      "can include:\n",
      "\n",
      "•  Supervised Fine-Tuning (SFT): Supervised fine-\n",
      "\n",
      "tuning using data annotated across helpfulness \n",
      "\n",
      "•  Don’t Blame the Annotator: Bias Already Starts in \n",
      "\n",
      "and safety. \n",
      "\n",
      "the Annotation Instructions\n",
      "\n",
      "•  Reinforcement Learning from Human Feedback \n",
      "\n",
      "•  Annotators with Attitudes: How Annotator Beliefs \n",
      "\n",
      "(RLHF) or AI Feedback (RLAIF): Training safety \n",
      "\n",
      "And Identities Bias Toxic Language Detection\n",
      "\n",
      "and helpfulness reward models to support \n",
      "\n",
      "There are several other risks to consider, such as \n",
      "\n",
      "overfitting, privacy, and security. To mitigate these \n",
      "\n",
      "RLHF techniques iteratively improves models \n",
      "\n",
      "and makes them more robust to jailbreaking \n",
      "\n",
      "risks, carefully design the fine-tuning process by \n",
      "\n",
      "techniques.\n",
      "\n",
      "curating a high-quality dataset that is representative \n",
      "\n",
      "•  Targeted Safety Context Distillation: Context \n",
      "\n",
      "of your use case, conduct rigorous evaluations, and \n",
      "\n",
      "distillation for safety helps the model associate \n",
      "\n",
      "test your fine-tuned model’s potential use via red \n",
      "\n",
      "adversarial prompts with safe responses by \n",
      "\n",
      "teaming (covered in step four - Evaluate and  \n",
      "\n",
      "prefixing a safe preprompt such as “You are a \n",
      "\n",
      "improve performance).\n",
      "\n",
      "safe and responsible assistant” to the adversarial \n",
      "\n",
      "prompt, followed by fine-tuning on new outputs.\n",
      "\n",
      "11\n",
      "\n",
      "April 2024AI at Meta\fTHE RESPONSIBLE FINE-TUNING FLOW\n",
      "\n",
      "Reinforcement Learning from Human  \n",
      "\n",
      "performance and safety, and iterating until satisfied \n",
      "\n",
      "Feedback (RLHF)\n",
      "\n",
      "with the model’s performance using holdout test \n",
      "\n",
      "To align the output of LLMs with user expectations \n",
      "\n",
      "datasets.\n",
      "\n",
      "and values, one approach that developers should \n",
      "\n",
      "There are many complementary types of evaluations \n",
      "\n",
      "consider is implementing Reinforcement Learning \n",
      "\n",
      "that are useful for measuring risks in models, \n",
      "\n",
      "from Human Feedback (RLHF) mechanisms. This \n",
      "\n",
      "including automatic benchmarks, manual annotations \n",
      "\n",
      "involves collecting ranking data from trained \n",
      "\n",
      "by human raters, and evaluations using an LLM \n",
      "\n",
      "annotators or users (given a model input and several \n",
      "\n",
      "itself as a rater. The Holistic Evaluation of Language \n",
      "\n",
      "generated outputs, ranking them from best to worst \n",
      "\n",
      "Models discusses some of the most commonly used \n",
      "\n",
      "according to policies), training a reward or helpfulness \n",
      "\n",
      "automatic benchmarks. As the industry matures, we \n",
      "\n",
      "model to act as a proxy of human feedback, and \n",
      "\n",
      "are excited for evaluation platforms to emerge to \n",
      "\n",
      "then optimizing the LLM to maximize the reward/\n",
      "\n",
      "help drive safety standardization, such as through \n",
      "\n",
      "helpfulness model score with reinforcement learning.  \n",
      "\n",
      "the MLCommons AI Safety working group. Evaluation \n",
      "\n",
      "strategies and processes to improve performance can \n",
      "\n",
      "Reinforcement Learning from AI  \n",
      "\n",
      "include: \n",
      "\n",
      "Feedback (RLAIF)\n",
      "\n",
      "•  Automatic evaluation leverages automatic \n",
      "\n",
      "Reward models can also be improved and tailored to \n",
      "\n",
      "benchmarks and classifiers to judge the output  \n",
      "\n",
      "specific policies by using Reinforcement Learning \n",
      "\n",
      "with respect to a specific category of risk.\n",
      "\n",
      "from AI Feedback (RLAIF). The fine-tuned LLM itself \n",
      "\n",
      "can be used to create synthetic ranking data for \n",
      "\n",
      "•  Manual evaluation leverages human annotators \n",
      "\n",
      "or subject matter experts to judge the model’s \n",
      "\n",
      "reward model training. Given a model input, response \n",
      "\n",
      "output. \n",
      "\n",
      "pairs and relevant guidelines, the LLM predicts \n",
      "\n",
      "which response would best follow the guidelines. \n",
      "\n",
      "The synthetic reward modeling data are then used to \n",
      "\n",
      "augment the reward model’s training data.\n",
      "\n",
      "•  Red teaming is a systematic effort to identify \n",
      "\n",
      "model vulnerabilities or emergent risks by \n",
      "\n",
      "crafting prompts that may elicit undesirable \n",
      "\n",
      "behavior or outputs. This type of manipulation \n",
      "\n",
      "of the model can be used to test safeguards and \n",
      "\n",
      "Step 3: Evaluate and improve performance\n",
      "\n",
      "attempts to “jailbreak” the model. \n",
      "\n",
      "The final stage is to evaluate the fine-tuned model on \n",
      "\n",
      "a test set to measure its performance on the specific \n",
      "\n",
      "task and against safety benchmarks, according to \n",
      "\n",
      "the use case. This includes analyzing the model’s \n",
      "\n",
      "strengths and weaknesses based on evaluation \n",
      "\n",
      "results, gathering more data to further enhance \n",
      "\n",
      "12\n",
      "\n",
      "April 2024AI at Meta \n",
      "\fRed teaming best practices \n",
      "\n",
      "Red teams should adopt systematic approaches  \n",
      "\n",
      "to testing and measurement, while estimating  \n",
      "\n",
      "real-world behaviors and threat vectors to the  \n",
      "\n",
      "extent possible.\n",
      "\n",
      "•  Regular testing: The model should undergo \n",
      "\n",
      "regular testing to determine whether or not \n",
      "\n",
      "mitigations against attacks are effective. \n",
      "\n",
      "This requires some form of automated \n",
      "\n",
      "evaluation, either with human labeling, which \n",
      "\n",
      "can be expensive, or with classifiers trained \n",
      "\n",
      "•  Diversity: Red teams should include a diverse \n",
      "\n",
      "to recognize responses that fall under the \n",
      "\n",
      "set of people from a range of professional \n",
      "\n",
      "risk categories.\n",
      "\n",
      "backgrounds that are representative of a broad \n",
      "\n",
      "group of potential users and demographics. Red \n",
      "\n",
      "teams can be composed of internal employees, \n",
      "\n",
      "experts, or community members.\n",
      "\n",
      "•  Subject matter expertise: Subject matter experts \n",
      "\n",
      "should judge model responses based on their \n",
      "\n",
      "familiarity with the identified risk categories and \n",
      "\n",
      "label responses that fall under each category. \n",
      "\n",
      "13\n",
      "\n",
      "April 2024AI at Meta\f3\n",
      "\n",
      "System-level alignment\n",
      "\n",
      "Without proper safeguards at the input and output \n",
      "\n",
      "levels, it is hard to ensure that the model will respond \n",
      "\n",
      "properly to adversarial inputs and will be protected \n",
      "\n",
      "from efforts to circumvent content policies and \n",
      "\n",
      "safeguard measures (“jailbreaking”). Mitigations at \n",
      "\n",
      "the output level can also act as a safeguard against \n",
      "\n",
      "generating high-risk or policy-violating content. \n",
      "\n",
      "Enforcement of content policies can be managed \n",
      "\n",
      "through automated systems and manual analysis \n",
      "\n",
      "of samples and reports. Automated systems may \n",
      "\n",
      "include machine learning and rule-based classifiers \n",
      "\n",
      "for filtering prompt inputs or system outputs. Usage \n",
      "\n",
      "Privacy adversarial attacks\n",
      "\n",
      "Additional privacy protections should be considered \n",
      "\n",
      "when releasing the product, to test whether bad \n",
      "\n",
      "actors may be able to improperly extract information. \n",
      "\n",
      "A privacy adversarial attack is a method where \n",
      "\n",
      "attackers can exfiltrate data from a model. For \n",
      "\n",
      "example, common adversarial attacks may include \n",
      "\n",
      "membership inference attacks on a model to predict \n",
      "\n",
      "whether or not a particular sample was in the training \n",
      "\n",
      "data, or model inversion attacks to reconstruct \n",
      "\n",
      "representative views of a subset of examples. \n",
      "\n",
      "Prompt injection attacks are attempts to circumvent \n",
      "\n",
      "content restrictions to produce particular outputs. \n",
      "\n",
      "A red team privacy adversarial attack conducted by a \n",
      "\n",
      "or consequence policies may be defined for when \n",
      "\n",
      "company may be able to demonstrate the feasibility \n",
      "\n",
      "users repeatedly violate those policies.\n",
      "\n",
      "of such attacks. In scenarios where companies \n",
      "\n",
      "fine-tune models using personal data (pursuant to \n",
      "\n",
      "applicable privacy laws), they should consider testing \n",
      "\n",
      "the outputs to see if the model memorized particular \n",
      "\n",
      "data. This approach may be especially useful for \n",
      "\n",
      "testing models that are intended to be deployed as \n",
      "\n",
      "AI assistants or agents.\n",
      "\n",
      "Enforcement of content \n",
      "\n",
      "policies can be managed \n",
      "\n",
      "through automated systems \n",
      "\n",
      "and manual analysis of \n",
      "\n",
      "samples and reports. \n",
      "\n",
      "Automated systems may include machine learning \n",
      "\n",
      "and rule-based classifiers for filtering prompt inputs \n",
      "\n",
      "or system outputs. Usage or consequence policies \n",
      "\n",
      "may be defined for when users repeatedly violate \n",
      "\n",
      "those policies.\n",
      "\n",
      "14\n",
      "\n",
      "April 2024AI at Meta\fMitigating risks at the input level\n",
      "\n",
      "•  Prompt engineering: Direct modifications of \n",
      "\n",
      "The input refers to the information provided by \n",
      "\n",
      "the user and passed to the system. The developer \n",
      "\n",
      "does not control what the user inputs. Without \n",
      "\n",
      "implementation of input filters and safeguards, even \n",
      "\n",
      "advanced models can potentially be manipulated to \n",
      "\n",
      "generate harmful or misleading outputs or violate \n",
      "\n",
      "content policies. Although safeguards to protect \n",
      "\n",
      "privacy and prevent potential harm can be developed \n",
      "\n",
      "by tuning the model, it should be expected that even \n",
      "\n",
      "after rigorous design and testing, those safeguards \n",
      "\n",
      "will not have perfect performance and may be \n",
      "\n",
      "subverted. Additional safeguards include direct \n",
      "\n",
      "filtering and engineering of the inputs. For these to \n",
      "\n",
      "be effective, model inputs must be well-formatted. \n",
      "\n",
      "These approaches include:\n",
      "\n",
      "•  Prompt filters: Even when inputs may not \n",
      "\n",
      "violate content policies, the model may produce \n",
      "\n",
      "problematic engagements or outputs. In these \n",
      "\n",
      "cases, it may be appropriate to filter, block, and \n",
      "\n",
      "hard code responses for some inputs until the \n",
      "\n",
      "model can respond in the intended way. This \n",
      "\n",
      "tactic may come with tradeoffs to the user’s \n",
      "\n",
      "experience and agency in engaging with the \n",
      "\n",
      "system. Thus, the safety benefits of such \n",
      "\n",
      "restrictions or modifications should be weighed \n",
      "\n",
      "against those costs, until more robust solutions \n",
      "\n",
      "are developed.\n",
      "\n",
      "the user inputs are an option for guiding the \n",
      "\n",
      "model behavior and encouraging responsible \n",
      "\n",
      "outputs, by including contextual information or \n",
      "\n",
      "constraints in the prompts to establish background \n",
      "\n",
      "knowledge and guidelines while generating the \n",
      "\n",
      "output. Modifications may be done in a variety \n",
      "\n",
      "of ways, such as with automated identification \n",
      "\n",
      "and categorization, assistance of the LLM itself, \n",
      "\n",
      "or rules engines. These can help improve the \n",
      "\n",
      "user experience by creating more diversity and \n",
      "\n",
      "expressiveness from the model. For example, \n",
      "\n",
      "prompt engineering can be leveraged to direct the \n",
      "\n",
      "model to include more diverse references or apply \n",
      "\n",
      "a certain tone or point of view. Prompt engineering \n",
      "\n",
      "rules may be hard coded or probabilistic. \n",
      "\n",
      "Alongside prompts, it \n",
      "\n",
      "might be beneficial to \n",
      "\n",
      "provide instructive sample \n",
      "\n",
      "inputs and outputs that \n",
      "\n",
      "illustrate the desired \n",
      "\n",
      "responsible behavior. \n",
      "\n",
      "15\n",
      "\n",
      "April 2024AI at Meta\fMitigating risks at the output level\n",
      "\n",
      "unreasonably restrict the usage of your model. \n",
      "\n",
      "Based on the downstream use case, you can apply \n",
      "\n",
      "several approaches for detecting and filtering the \n",
      "\n",
      "generated output of models for problematic or policy-\n",
      "\n",
      "violating content. Here are some considerations and \n",
      "\n",
      "best practices for filtering outputs. Any output filter \n",
      "\n",
      "Words often have context-dependent meanings, \n",
      "\n",
      "and terms that could be sexually suggestive, for \n",
      "\n",
      "example, may also be used in medical contexts. \n",
      "\n",
      "Content policies will help articulate the specifics \n",
      "\n",
      "between permitted and prohibited topics to users.\n",
      "\n",
      "mitigation should include all languages that are used \n",
      "\n",
      "•  Classifiers: The more effective, but also more \n",
      "\n",
      "in the region where your product is available.\n",
      "\n",
      "difficult, approach is to develop classifiers that \n",
      "\n",
      "•  Blocklists: One of the easiest ways to prevent the \n",
      "\n",
      "generation of high-risk content is to compile a \n",
      "\n",
      "list of all the phrases that your model should not, \n",
      "\n",
      "under any circumstances, be permitted to include \n",
      "\n",
      "in a response. Many words are easily identifiable \n",
      "\n",
      "as problematic; slurs, for example, are typically \n",
      "\n",
      "offensive no matter their context. While blocklists \n",
      "\n",
      "are attractive for their simplicity, they may \n",
      "\n",
      "detect and filter outputs based on the meaning \n",
      "\n",
      "conveyed by the words chosen. Classifiers, \n",
      "\n",
      "when properly trained on known examples of a \n",
      "\n",
      "particular sentiment or type of semantic content, \n",
      "\n",
      "can become highly effective at identifying novel \n",
      "\n",
      "instances in which that sentiment or meaning  \n",
      "\n",
      "is expressed.\n",
      "\n",
      "16\n",
      "\n",
      "April 2024AI at Meta\f4\n",
      "\n",
      "Build transparency and reporting \n",
      "mechanisms in user interactions\n",
      "\n",
      "Releasing an LLM-powered feature for users to \n",
      "\n",
      "interact with can reveal new use cases as well as \n",
      "\n",
      "new concerns. User interactions can provide critical \n",
      "\n",
      "feedback, which can be used for reinforcement \n",
      "\n",
      "learning (discussed in a previous section). This is \n",
      "\n",
      "also an opportunity to provide appropriate notice, \n",
      "\n",
      "transparency, and control to users, which can lead to \n",
      "\n",
      "greater satisfaction and trust in the feature. \n",
      "\n",
      "Feedback & reporting mechanisms \n",
      "\n",
      "Evaluate effectiveness \n",
      "\n",
      "While prompt filtering and engineering are critical \n",
      "\n",
      "safety mitigations, it’s important to monitor \n",
      "\n",
      "effectiveness and avoid unintended consequences. \n",
      "\n",
      "Some best practices include:\n",
      "\n",
      "•  Test for unintended outcomes. Take  \n",
      "\n",
      "caution that prompt engineering doesn’t \n",
      "\n",
      "inadvertently create other issues. Test  \n",
      "\n",
      "end-to-end performance after any prompt \n",
      "\n",
      "engineering to ensure desired behavior.\n",
      "\n",
      "•  Evaluate effectiveness of safeguards. Many \n",
      "\n",
      "publicly available datasets offer collections  \n",
      "\n",
      "of prompts that are designed to benchmark \n",
      "\n",
      "Facilitating user interaction with appropriate \n",
      "\n",
      "against specific concerns when used as inputs. \n",
      "\n",
      "feedback or reporting mechanisms is key to ensuring \n",
      "\n",
      "After model responses are collected, they can  \n",
      "\n",
      "quality output. Feedback mechanisms can be as \n",
      "\n",
      "be evaluated by using standardized metrics. \n",
      "\n",
      "simple as positive or negative (thumbs up or thumbs \n",
      "\n",
      "•  Adjust for different languages. Prompt  \n",
      "\n",
      "filtering and engineering mitigations should \n",
      "\n",
      "include all languages that are used in the  \n",
      "\n",
      "region where your product is available; the \n",
      "\n",
      "effectiveness of these mitigations may be \n",
      "\n",
      "dependent on linguistic and community-level \n",
      "\n",
      "nuances. Llama was trained primarily on data  \n",
      "\n",
      "in English, in accordance with its intended  \n",
      "\n",
      "use, so it is critical to carefully evaluate any \n",
      "\n",
      "mitigations in other languages.  \n",
      "\n",
      "down), and tailoring feedback to the types of issues \n",
      "\n",
      "that may be foreseeable based on a company’s use \n",
      "\n",
      "case (for example, AI assistants) can enhance the \n",
      "\n",
      "quality of feedback. This feedback can be used by \n",
      "\n",
      "developers to improve the model in more targeted \n",
      "\n",
      "ways. Providing an option for freeform feedback \n",
      "\n",
      "within a reporting mechanism can also reveal new or \n",
      "\n",
      "unanticipated concerns raised by users. Furthermore, \n",
      "\n",
      "users can identify and highlight errors, unsafe \n",
      "\n",
      "behaviors, or suboptimal actions that the model \n",
      "\n",
      "might not recognize on its own. Developers can \n",
      "\n",
      "further train the model with this feedback to improve \n",
      "\n",
      "performance and avoid repeating mistakes. Product \n",
      "\n",
      "17\n",
      "\n",
      "April 2024AI at Meta\fdevelopers should review feedback by monitoring the \n",
      "\n",
      "critical factors in ascertaining when and how \n",
      "\n",
      "rate that users report model outputs and by manually \n",
      "\n",
      "to be transparent. Work with your appropriate \n",
      "\n",
      "reviewing those reports and selected samples of \n",
      "\n",
      "advisors to determine the types of transparency \n",
      "\n",
      "model outputs. \n",
      "\n",
      "Transparency & control best practices \n",
      "\n",
      "To ensure high-quality feedback and provide end \n",
      "\n",
      "users with notice and choice about their interactions \n",
      "\n",
      "with your AI assets, developers should consider the \n",
      "\n",
      "following practices for user interactions: \n",
      "\n",
      "•  Transparency: Developers should consider ways \n",
      "\n",
      "to provide transparency to end users regarding \n",
      "\n",
      "that should be provided to users, including \n",
      "\n",
      "whether users should be informed that their \n",
      "\n",
      "responses may be used to fine-tune a model. \n",
      "\n",
      "Developers should also consider the use of \n",
      "\n",
      "system cards to provide insight into their AI \n",
      "\n",
      "system’s underlying architecture and explain how \n",
      "\n",
      "a particular AI experience is produced. Further \n",
      "\n",
      "best practices are outlined in the Partnership on \n",
      "\n",
      "AI’s Responsible Practices for Synthetic Media. \n",
      "\n",
      "potential risks and limitations of the system \n",
      "\n",
      "•  Control mechanisms: Additional controls could \n",
      "\n",
      "prior to or at the time of user interaction. For \n",
      "\n",
      "include giving users the option to customize the \n",
      "\n",
      "instance, notice to users that they are interacting \n",
      "\n",
      "outputs generated by an LLM. For example, a \n",
      "\n",
      "with an AI-powered chatbot may increasingly \n",
      "\n",
      "user could select or reject outputs from a list of \n",
      "\n",
      "be required in certain markets, and is a best \n",
      "\n",
      "multiple options. Offering editing capabilities \n",
      "\n",
      "practice to address concerns that may be related \n",
      "\n",
      "can also enhance a user’s sense of agency \n",
      "\n",
      "to false or incorrect information. Developers \n",
      "\n",
      "over outputs, and developers should consider \n",
      "\n",
      "should neither claim nor imply that an AI agent is \n",
      "\n",
      "education flows that can set a user up for \n",
      "\n",
      "human, especially when building and deploying \n",
      "\n",
      "success, such as offering prompt suggestions or \n",
      "\n",
      "anthropomorphized interfaces. Context, intent, \n",
      "\n",
      "explanations of how to improve an output. \n",
      "\n",
      "sensitivity, and likelihood to deceive are additional \n",
      "\n",
      "18\n",
      "\n",
      "April 2024AI at Meta\fResources for developers\n",
      "\n",
      "There is a value chain emerging to support the \n",
      "\n",
      "Filters and classifiers:\n",
      "\n",
      "responsible training and use of LLMs, which we \n",
      "\n",
      "believe will be advanced through more open \n",
      "\n",
      "releases and sharing of best practices, tools, and \n",
      "\n",
      "benchmarking. A growing number of researchers, \n",
      "\n",
      "platforms, companies, and developer communities \n",
      "\n",
      "are contributing to this ecosystem. We expect more \n",
      "\n",
      "tools for the responsible development of LLM to \n",
      "\n",
      "become available over time and are committed to \n",
      "\n",
      "•  Meta Llama Guard and other solutions\n",
      "\n",
      "•  Content-filtering systems from Azure, supporting \n",
      "\n",
      "a range of languages: learn.microsoft.com/\n",
      "\n",
      "en-us/azure/cognitive-services/content-safety/\n",
      "\n",
      "overview \n",
      "\n",
      "•  Filter lists for generation of problematic words: \n",
      "\n",
      "github.com/LDNOOBW/naughty-words-js \n",
      "\n",
      "fostering more open exchange of safety research and \n",
      "\n",
      "•  Recipes for safety in open-domain Chatbots, \n",
      "\n",
      "tools to support  developers. \n",
      "\n",
      "including a sensitive topics classifier: parl.ai/\n",
      "\n",
      "projects/safety_recipes/  \n",
      "\n",
      "It is critical to remain aware  \n",
      "\n",
      "of the latest versions of \n",
      "\n",
      "models and use the most \n",
      "\n",
      "current version to get the \n",
      "\n",
      "best results. \n",
      "\n",
      "Platforms for tools and evaluations:\n",
      "\n",
      "•  MLCommons AI Safety (v0.5): mlcommons.\n",
      "\n",
      "org/2024/04/mlc-aisafety-v0-5-poc/\n",
      "\n",
      "•  Meta Llama Cybersecurity evaluations\n",
      "\n",
      "•  Benchmarking of LLMs by Stanford’s Center for \n",
      "\n",
      "Research on Foundation Models, HELM: crfm.\n",
      "\n",
      "Our partnership to make Llama available on the Azure \n",
      "\n",
      "stanford.edu/helm/latest/\n",
      "\n",
      "Model Catalog will enable developers using Microsoft \n",
      "\n",
      "•  EleutherAI LLM Evaluation Harness: github.com/\n",
      "\n",
      "Azure to leverage their cloud-native tools for content \n",
      "\n",
      "EleutherAI/lm-evaluation-harness \n",
      "\n",
      "filtering and safety features. Below, we provide a \n",
      "\n",
      "•  Hugging Face Hub which hosts open source \n",
      "\n",
      "few notable hubs and implementation resources for \n",
      "\n",
      "models, datasets, and is a space for developers \n",
      "\n",
      "developers, but this list is not exhaustive. Microsoft \n",
      "\n",
      "to share safeguards and access benchmarking \n",
      "\n",
      "also offers a repository of Responsible AI Resources. \n",
      "\n",
      "information: huggingface.co/docs/hub/index \n",
      "\n",
      "19\n",
      "\n",
      "April 2024AI at Meta\fReporting resources\n",
      "\n",
      "If you have any information about issues, violations, \n",
      "\n",
      "•  Reporting bugs and security concerns:  \n",
      "\n",
      "facebook.com/whitehat/info\n",
      "\n",
      "or problems, please help keep our communities safe \n",
      "\n",
      "•  Reporting violations of the Acceptable  \n",
      "\n",
      "by using our reporting resources.\n",
      "\n",
      "•  Reporting issues with the model: github.com/\n",
      "\n",
      "facebookresearch/llama\n",
      "\n",
      "•  Reporting risky content generated by \n",
      "\n",
      "the model: developers.facebook.com/\n",
      "\n",
      "llama_output_feedback\n",
      "\n",
      "Use Policy or unlicensed uses of Llama:  \n",
      "\n",
      "LlamaUseReport@meta.com\n",
      "\n",
      "20\n",
      "\n",
      "April 2024AI at Meta\fCombining the components \n",
      "of responsible generative AI\n",
      "\n",
      "Each stage of model development presents \n",
      "\n",
      "data-collection stage to user feedback, be sure  \n",
      "\n",
      "opportunities to enhance the safety of your AI \n",
      "\n",
      "to keep your overall goal in mind.\n",
      "\n",
      "feature. However, it’s crucial to acknowledge the \n",
      "\n",
      "interconnectedness of these stages and how the \n",
      "\n",
      "decisions made at each stage can impact others. \n",
      "\n",
      "Building a responsible AI ecosystem requires ongoing \n",
      "\n",
      "efforts to refine each component and ensure they \n",
      "\n",
      "work together effectively. \n",
      "\n",
      "Here are some key considerations for implementing \n",
      "\n",
      "these components in unison:\n",
      "\n",
      "•  Holistic optimization. Although each component \n",
      "\n",
      "has a specific role and optimization goal, \n",
      "\n",
      "components are not isolated entities. Over-\n",
      "\n",
      "optimization of one component without \n",
      "\n",
      "considering its interaction with others can lead \n",
      "\n",
      "to suboptimal outcomes. For instance, over-\n",
      "\n",
      "filtering training data for safety might make \n",
      "\n",
      "later fine-tuning less effective, as the model \n",
      "\n",
      "may not recognize and handle unsafe content \n",
      "\n",
      "•  Standardizing processes for learning from \n",
      "\n",
      "feedback/errors. Embracing an iterative model-\n",
      "\n",
      "development mindset is crucial. Establish a well-\n",
      "\n",
      "defined process for incorporating new learnings \n",
      "\n",
      "into subsequent model training. This process \n",
      "\n",
      "should include consistent feedback analysis, \n",
      "\n",
      "prioritization of identified issues, and systematic \n",
      "\n",
      "application of learnings in the next iteration of \n",
      "\n",
      "model training.\n",
      "\n",
      "The field of generative AI is complex, ever-evolving, \n",
      "\n",
      "and full of potential, but it’s not without risks. The \n",
      "\n",
      "key to unlocking its benefits while mitigating the \n",
      "\n",
      "downsides is responsible AI practice. This practice \n",
      "\n",
      "starts with understanding the complexities of the \n",
      "\n",
      "technology, the potential impacts on users and \n",
      "\n",
      "society, and the importance of continuously striving \n",
      "\n",
      "for improvement. \n",
      "\n",
      "appropriately. This is why different layers of \n",
      "\n",
      "By embracing the principles of transparency, \n",
      "\n",
      "safety mitigations throughout the development \n",
      "\n",
      "accountability and user empowerment, as well \n",
      "\n",
      "lifecycle are critical for creating high-performing, \n",
      "\n",
      "as having a commitment to ongoing learning and \n",
      "\n",
      "responsible products.\n",
      "\n",
      "•  Alignment of objectives at each stage of \n",
      "\n",
      "development. To yield a product that is optimized \n",
      "\n",
      "for your target use cases, it’s essential to have \n",
      "\n",
      "a consistent set of goals and outcomes that \n",
      "\n",
      "guide each stage of the process. From the \n",
      "\n",
      "improvement, you can ensure that your AI feature  \n",
      "\n",
      "is not only innovative and useful but also responsible \n",
      "\n",
      "and respectful. We hope this guide serves as a \n",
      "\n",
      "valuable tool in your journey toward responsible  \n",
      "\n",
      "AI practice.\n",
      "\n",
      "21\n",
      "\n",
      "April 2024AI at Meta\fADDENDUM\n",
      "\n",
      "Introducing Code Llama\n",
      "\n",
      "Code Llama is a set of large language models for \n",
      "\n",
      "code with known insecure patterns or complying \n",
      "\n",
      "code based on Llama 2 providing strong code \n",
      "\n",
      "with a cyber attacker’s request. Measuring the \n",
      "\n",
      "generation and infilling capabilities, support for \n",
      "\n",
      "model’s response to malicious requests allowed \n",
      "\n",
      "large input contexts, and zero-shot instruction \n",
      "\n",
      "us to implement mitigations which make Code \n",
      "\n",
      "following ability for programming tasks, as well as \n",
      "\n",
      "Llama 70B safer than other available models, while \n",
      "\n",
      "state-of-the-art base models for fine-tuning. We \n",
      "\n",
      "remaining helpful. For detailed information on model \n",
      "\n",
      "provide multiple flavors to cover a wide range of \n",
      "\n",
      "training, architecture and parameters, evaluations, \n",
      "\n",
      "applications: foundation code models (Code Llama), \n",
      "\n",
      "responsible AI and safety refer to our research paper \n",
      "\n",
      "Python specializations (Code Llama - Python), and \n",
      "\n",
      "and Responsible Use Guide. \n",
      "\n",
      "instruction-following models (Code Llama - Instruct) \n",
      "\n",
      "with 7B, 13B, 34B and 70B parameters each. All \n",
      "\n",
      "models are trained on sequences of 16k tokens \n",
      "\n",
      "and show improvements on inputs with up to 100k \n",
      "\n",
      "tokens. The 7B and 13B Code Llama and Code \n",
      "\n",
      "Llama - Instruct variants support infilling based on \n",
      "\n",
      "surrounding content. Code Llama was developed \n",
      "\n",
      "by training Llama 2 on publicly available code and \n",
      "\n",
      "natural language datasets related to code. \n",
      "\n",
      "Building AI models responsibly is important to \n",
      "\n",
      "us, and we undertook a variety of safety measures \n",
      "\n",
      "before releasing Code Llama, including many of \n",
      "\n",
      "the measures used for Llama 2. Before releasing \n",
      "\n",
      "the 7B, 13B, and 34B variants in August 2023, we \n",
      "\n",
      "This addendum to the guide \n",
      "\n",
      "outlines additional, coding-\n",
      "\n",
      "specific best practices that \n",
      "\n",
      "developers should consider  \n",
      "\n",
      "in responsible development \n",
      "\n",
      "of downstream coding-\n",
      "\n",
      "related features. \n",
      "\n",
      "Code Llama potential use cases \n",
      "\n",
      "asked cybersecurity and malware development \n",
      "\n",
      "Code Llama has two broad use case categories:\n",
      "\n",
      "experts to evaluate the Code Llama model’s capacity \n",
      "\n",
      "for enabling malware development by otherwise \n",
      "\n",
      "unskilled adversaries. We subsequently expanded \n",
      "\n",
      "our evaluation suite to include Meta’s CyberSecEval, \n",
      "\n",
      "which evaluates an LLM’s capacity for producing \n",
      "\n",
      "1.  Foundation model use case: Train on more data \n",
      "\n",
      "to create variants of Code Llama, e.g, add other \n",
      "\n",
      "programming languages (C++, Java), increase \n",
      "\n",
      "context length, reduce latency by quantization  \n",
      "\n",
      "of model, etc.\n",
      "\n",
      "22\n",
      "\n",
      "April 2024AI at Meta\fADDENDUM\n",
      "\n",
      "2. \n",
      "\n",
      "Instruction model use case: Add more instruction \n",
      "\n",
      "code-specific best practices when building on top of \n",
      "\n",
      "data to improve the model’s ability to follow \n",
      "\n",
      "Code Llama in line with their specific use case.\n",
      "\n",
      "instructions, extend capability to understand \n",
      "\n",
      "non-English instructions, create standalone \n",
      "\n",
      "code bots and integrate into existing 3rd party \n",
      "\n",
      "products, etc.\n",
      "\n",
      "Define content policies for use case\n",
      "\n",
      "•  A content policy defines what content is allowable \n",
      "\n",
      "based on the intended use and deployment \n",
      "\n",
      "context. This may also outline limitations on \n",
      "\n",
      "Both the foundation and instruction Code Llama \n",
      "\n",
      "producing potentially problematic  content. In \n",
      "\n",
      "models are not designed to be used as a large \n",
      "\n",
      "the code domain, models should avoid producing \n",
      "\n",
      "language model for general purpose - for such \n",
      "\n",
      "malware, virus, or malicious code. Developers \n",
      "\n",
      "scenarios refer to Llama 2. \n",
      "\n",
      "1\n",
      "\n",
      "Foundation model use case\n",
      "\n",
      "This model could be used for further research \n",
      "\n",
      "exploration on specialized foundation large \n",
      "\n",
      "language models for programming. When \n",
      "\n",
      "fine-tuning Code Llama, we refer users to the \n",
      "\n",
      "Responsible Use Guide for Llama 2, which provides \n",
      "\n",
      "essential guidance on responsible development \n",
      "\n",
      "of downstream models, including on (i) defining \n",
      "\n",
      "content policies and mitigations; (ii) preparing \n",
      "\n",
      "data; (iii) fine-tuning the model; (iv) evaluating and \n",
      "\n",
      "improving performance; (v) addressing input- and \n",
      "\n",
      "output-level risks; and (vi) building transparency \n",
      "\n",
      "and reporting mechanisms in user interactions.\n",
      "\n",
      "Additionally, developers should consider \n",
      "\n",
      "should consider how bad actors prompt the \n",
      "\n",
      "model to produce these results. \n",
      "\n",
      "•  These policies will dictate the data needed, \n",
      "\n",
      "annotation requirement, and goals of safety  \n",
      "\n",
      "fine-tuning. They may also be applied in input- \n",
      "\n",
      "and output-level safeguards as additional  \n",
      "\n",
      "safety mechanisms. \n",
      "\n",
      "Evaluations & benchmarks\n",
      "\n",
      "•  Models should be evaluated against their \n",
      "\n",
      "intended use and end user requirements. \n",
      "\n",
      "Specifically, code models should be evaluated \n",
      "\n",
      "against code-specific benchmarks. As a resource, \n",
      "\n",
      "you can find various benchmarks on Papers with \n",
      "\n",
      "Code: Code Generation Benchmarks.\n",
      "\n",
      "•  We recommend evaluating the cybersecurity \n",
      "\n",
      "safety of coding models with the CyberSecEval \n",
      "\n",
      "(GitHub) which was released as part of our open \n",
      "\n",
      "trust and safety tools and evaluations project.\n",
      "\n",
      "•  Non code-specific safety evaluations are also \n",
      "\n",
      "recommended, for example, code models can be \n",
      "\n",
      "evaluated on benchmarks such as TruthfulQA, \n",
      "\n",
      "ToxiGen and BOLD. \n",
      "\n",
      "23\n",
      "\n",
      "April 2024AI at Meta\fADDENDUM\n",
      "\n",
      "Red teaming & fine-tuning considerations\n",
      "\n",
      "• \n",
      "\n",
      "If the model’s output will be used in production \n",
      "\n",
      "•  The data should be representative of the  \n",
      "\n",
      "end users’ requirements. For example, if the \n",
      "\n",
      "model is meant for Javascript generation,  \n",
      "\n",
      "the dataset chosen to fine-tune with should \n",
      "\n",
      "be Javascript-focused. Developers should also \n",
      "\n",
      "consider examining and placing restrictions  \n",
      "\n",
      "systems, developers should ensure the code that \n",
      "\n",
      "the model is trained on is free of relevant security \n",
      "\n",
      "vulnerabilities. Developers and end-users that \n",
      "\n",
      "use the model as an assistant for software \n",
      "\n",
      "development should  continue to follow security \n",
      "\n",
      "best practices.\n",
      "\n",
      "on any potentially malicious or nefarious code  \n",
      "\n",
      "System-level safeguards\n",
      "\n",
      "in the data.\n",
      "\n",
      "•  As for other LLM use cases, we recommend \n",
      "\n",
      "•  Developers should ensure the security and \n",
      "\n",
      "adding appropriate system-level safeguards in \n",
      "\n",
      "robustness qualities of the training code dataset \n",
      "\n",
      "addition to the model-level built-in alignment. \n",
      "\n",
      "matches the security requirements of the output \n",
      "\n",
      "and the systems where the output code will be \n",
      "\n",
      "integrated based on a specific use case.\n",
      "\n",
      "•  As part of our open trust and safety tools, we \n",
      "\n",
      "released Code Shield, an insecure code detector \n",
      "\n",
      "that can be used at inference to prevent bad \n",
      "\n",
      "•  Developers should perform safety studies on \n",
      "\n",
      "coding practices. Visit Code Shield Github for \n",
      "\n",
      "code-specific areas such as intentional malware \n",
      "\n",
      "more information on how to integrate it into  \n",
      "\n",
      "generation and the unintentional introduction \n",
      "\n",
      "your application. \n",
      "\n",
      "of vulnerable code. Working with red-teaming \n",
      "\n",
      "domain experts can help developers evaluate \n",
      "\n",
      "the model’s capacity to lower the bar for writing \n",
      "\n",
      "malicious code when the prompt intent is clear \n",
      "\n",
      "and the output goes beyond resources already \n",
      "\n",
      "publicly available on the Internet. \n",
      "\n",
      "When using Code Llama in particular, it is important \n",
      "\n",
      "to keep in mind that Code Llama is specialized for \n",
      "\n",
      "code-related tasks and may not be appropriate as a \n",
      "\n",
      "foundation model for other task families, e.g, general \n",
      "\n",
      "language model. We note that for downstream tasks \n",
      "\n",
      "where the Python programming language is more \n",
      "\n",
      "•  Developers and end-users that use the model as \n",
      "\n",
      "relevant, it may be more appropriate to use our Code \n",
      "\n",
      "an assistant for software development should \n",
      "\n",
      "Llama - Python model variant. Code Llama and Code \n",
      "\n",
      "be aware of the model’s overall language safety. \n",
      "\n",
      "Llama - Python are not trained with instruction \n",
      "\n",
      "Performing safety studies and comparing results \n",
      "\n",
      "data hence are not designed to follow instruction in \n",
      "\n",
      "to representative benchmarks can identify \n",
      "\n",
      "natural language. Any use of these models to perform \n",
      "\n",
      "particular categories of content risk. To mitigate \n",
      "\n",
      "general natural language tasks is not recommended \n",
      "\n",
      "those risks, collect relevant fine-tuning data that \n",
      "\n",
      "to avoid potential misuse of the models.\n",
      "\n",
      "is not within the test set, and fine-tune the model \n",
      "\n",
      "by controlling for higher measured safety while \n",
      "\n",
      "maintaining helpfulness.\n",
      "\n",
      "24\n",
      "\n",
      "April 2024AI at Meta\fADDENDUM\n",
      "\n",
      "2Instruction model use case\n",
      "\n",
      "This model could be used for further applied \n",
      "\n",
      "research and testing of specialized large language \n",
      "\n",
      "models for programming. Code Llama - Instruct \n",
      "\n",
      "has the ability to understand instructions in \n",
      "\n",
      "natural language. When using Code Llama for code \n",
      "\n",
      "generation tasks, we recommend developers use \n",
      "\n",
      "our Code Llama - Instruct variants, which have \n",
      "\n",
      "been fine-tuned to generate helpful and safe \n",
      "\n",
      "answers to users. Consult the full Responsible  \n",
      "\n",
      "Use Guide for best practices with regards to \n",
      "\n",
      "generally addressing input- and output-level \n",
      "\n",
      "risks and building transparency and reporting \n",
      "\n",
      "mechanisms in user interactions, as relevant to  \n",
      "\n",
      "a specific use case.\n",
      "\n",
      "For both use cases, users must abide by our \n",
      "\n",
      "Acceptable Use Policy.\n",
      "\n",
      "Reporting resources for developers\n",
      "\n",
      "If you have any information about issues, \n",
      "\n",
      "violations, or problems, please help keep our \n",
      "\n",
      "communities safe by using our reporting resources.\n",
      "\n",
      "• \n",
      "\n",
      "Reporting issues with the model via our \n",
      "\n",
      "GitHub repo\n",
      "\n",
      "• \n",
      "\n",
      "Reporting risky content generated by the \n",
      "\n",
      "model via our Developer portal\n",
      "\n",
      "• \n",
      "\n",
      "Reporting bugs and security concerns via our \n",
      "\n",
      "Bug Bounty\n",
      "\n",
      "25\n",
      "\n",
      "April 2024AI at Meta\f\n"
     ]
    }
   ],
   "source": [
    "text = extract_text(path)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
